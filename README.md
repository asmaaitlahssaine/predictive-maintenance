
# ğŸš— Predictive Maintenance â€“ End-to-End MLOps Project

This project implements a complete **Predictive Maintenance Pipeline** for engine condition classification using real-like sensor data.  
It includes **data preprocessing**, **feature engineering**, **model training**, **MLflow experiment tracking**, **Streamlit dashboard**, **unit tests**, and **CI/CD automation**.

---

## ğŸ“ Project Structure

```

.
â”œâ”€â”€ app/                       # Streamlit dashboard
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw CSV dataset
â”‚   â””â”€â”€ processed/             # Processed dataset
â”œâ”€â”€ models/                    # Saved best model (.joblib)
â”œâ”€â”€ mlruns/                    # MLflow tracking data
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess.py          # Data cleaning + feature engineering
â”‚   â”œâ”€â”€ train.py               # Training pipeline (RF + XGBoost)
â”‚   â””â”€â”€ check_performance.py   # Automated model quality validation
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_preprocess.py     # Unit tests for feature engineering
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ debug_model.py
â””â”€â”€ README.md

```

---

## ğŸ§  Project Overview

This predictive maintenance system uses engine sensor data to classify the **Engine Condition** as:

- **0 = Normal**
- **1 = Abnormal**

The pipeline includes:

### âœ” Data Preprocessing
- Handling missing values  
- Cleaning sensor columns  
- Adding engineered features:
  - `Engine_power = Engine rpm Ã— Lub oil pressure`
  - `Temperature_difference = Coolant temp â€“ Lub oil temp`

### âœ” Model Training
Two models are trained:
- **Random Forest**
- **XGBoost**

Metrics used:
- **Accuracy**
- **ROC-AUC**  
(ROC-AUC is used to select the best model)

### âœ” MLflow Tracking
All experiments are logged:
- parameters  
- metrics  
- models  
- artifacts  

The best model is stored as:
```

models/best_model.joblib

````

### âœ” Streamlit Dashboard
A clean UI that allows:
- adjusting sensor values  
- generating predictions  
- visualizing derived features  

---

## â–¶ï¸ How to Run the Project

### **1ï¸âƒ£ Create a virtual environment**
```bash
python -m venv .venv
source .venv/bin/activate   # Linux/macOS
.venv\Scripts\activate      # Windows
````

### **2ï¸âƒ£ Install dependencies**

```bash
pip install -r requirements.txt
```

---

## ğŸ§¹ Run Preprocessing

```bash
python scripts/preprocess.py \
    --input data/raw/engine_data.csv \
    --output data/processed/processed.csv
```

---

## ğŸ¤– Train Models

```bash
python scripts/train.py \
    --input data/processed/processed.csv \
    --target "Engine Condition"
```

This generates:

* `models/best_model.joblib`
* MLflow experiment logs in `mlruns/`

---

## ğŸ§ª Run Tests

```bash
pytest -v
```

Includes tests for:

* feature creation
* missing column handling
* datatype validation
* edge cases

---

## ğŸ–¥ Launch the Streamlit Dashboard

```bash
streamlit run app/app.py
```

Features:

* interactive sensor sliders
* live prediction
* confidence score
* auto-calculation of engine power and temp difference

---

## ğŸ”§ CI/CD Pipeline (GitHub Actions)

A workflow automatically:

* executes preprocessing
* trains the model
* validates performance (ROC-AUC threshold)
* uploads:

  * the trained model
  * MLflow tracking folder
  * performance report

Located at:

```
.github/workflows/train.yml
```

---

## ğŸ“ˆ Example Results

* **Best model:** RandomForest
* **Accuracy:** ~65â€“68%
* **ROC-AUC:** ~0.67
* **Interpretation:**
  Dataset is simple and synthetic â†’ performance reasonable.

---

## ğŸ Conclusion

This project demonstrates:

* End-to-end ML pipeline
* MLOps practices (MLflow + CI/CD)
* Model training & evaluation
* Interactive dashboard
* Testing and reproducibility

It is suitable for **Data Engineer**, **AI Engineer**, and **MLOps** portfolios.

